{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qxfgINihUfV"
      },
      "source": [
        "#1. Importer DataSet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY8LvMn0hSL_",
        "outputId": "440dadc7-c585-4a5f-fcc4-97b605d5dfb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fm0P7MlhdGG"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Projet Master/DataSets/BaseDeDonneesV3_1.zip\" minutiae_detection.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO9lYiyOG1gQ",
        "outputId": "eeea463e-dec2-47f2-89ed-feafc8f2f856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deecompression .....\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Deecompression .....\")\n",
        "with ZipFile('minutiae_detection.zip') as zip:\n",
        "    zip.extractall('Dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB9nxXShjN6a"
      },
      "source": [
        "#2. Loads libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF10g-TDjRE6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Conv2D, Dense, Input, MaxPool2D, Dropout, Activation, Normalization, Flatten, Reshape\n",
        "from keras.activations import relu, softmax\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random \n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgP49YVqjj8h"
      },
      "source": [
        "#3. Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guHxYoZ0KG-c"
      },
      "outputs": [],
      "source": [
        "def generat_model(optimizer=None,input_shape=(),nb_classe=2, individual=[]):\n",
        "  \n",
        "  my_model = Sequential(name=\"Model\")\n",
        "  my_model.add(Input(shape=input_shape))\n",
        "  #my_model.add(Reshape((28,28,1)))\n",
        "\n",
        "  conv_layers = individual[0]\n",
        "  dense_layers = individual[1]\n",
        "  \n",
        "  for i,layer in enumerate(conv_layers):\n",
        "    my_model.add(Conv2D(layer[0], layer[1], name=f'conv{i}',padding=\"same\")) #Add conv parameters\n",
        "    if layer[3] == 1:  my_model.add(Activation(relu))\n",
        "    if layer[2] != 0: my_model.add(MaxPool2D((layer[2],layer[2]),padding=\"same\"))\n",
        "    \n",
        "\n",
        "  my_model.add(Flatten())\n",
        "\n",
        "  for i,layer in enumerate(dense_layers):\n",
        "    my_model.add(Dense(layer[0]))\n",
        "    if layer[1] == 1:  my_model.add(Activation(relu))\n",
        "\n",
        "  my_model.add(Dense(nb_classe,activation=softmax))\n",
        "\n",
        "  my_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return my_model\n",
        "\n",
        "def generat_model_2(optimizer=None,input_shape=(), individual=[]):\n",
        "  my_model = Sequential(name=\"Model\")\n",
        "  my_model.add(Input(shape=input_shape))\n",
        "  #my_model.add(Reshape((28,28,1)))\n",
        "\n",
        "  conv_layers = individual[0]\n",
        "  dense_layers = individual[1]\n",
        "  \n",
        "  for i,layer in enumerate(conv_layers):\n",
        "    my_model.add(Conv2D(layer[0], layer[1], name=f'conv{i}',padding=\"same\")) #Add conv parameters\n",
        "    if layer[3] == 1:  my_model.add(Activation(relu))\n",
        "    if layer[2] != 0: my_model.add(MaxPool2D((layer[2],layer[2]),padding=\"same\"))\n",
        "    \n",
        "\n",
        "  my_model.add(Flatten())\n",
        "\n",
        "  for i,layer in enumerate(dense_layers):\n",
        "    my_model.add(Dense(layer[0]))\n",
        "    if layer[1] == 1:  my_model.add(Activation(relu))\n",
        "\n",
        "  my_model.add(Dense(3))\n",
        "\n",
        "  my_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return my_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMvI7eySTF3"
      },
      "source": [
        "#4. Genetic Algorithme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XKPbuS6Beax"
      },
      "source": [
        "## Search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuUE9B7TK2fT"
      },
      "outputs": [],
      "source": [
        "nb_bloc_conv_valeus = [3,4,5]\n",
        "nb_filters_valeus = [32,64,128,256,512] #[10,20,30,40]\n",
        "filter_size_valeus = [3,5,7,9]\n",
        "nb_dense_layer_valeus = [1,2,3]\n",
        "nb_dense_units_valeus = [64,128,256,512]\n",
        "polling_size_valeus = [0,3,5] #Dans le cas ou c'est 0 on ajoute pas de pooling\n",
        "\n",
        "learning_rate = [0.1,0.01,0.02,0.001,0.002,0.0001] \n",
        "\n",
        "search_space_conv = [nb_filters_valeus,filter_size_valeus,polling_size_valeus,[0,1]]\n",
        "search_space_dense = [nb_dense_units_valeus,[0,1]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6mQIRyoxR27"
      },
      "source": [
        "## Population generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_vZR3UBxX8D"
      },
      "outputs": [],
      "source": [
        "def generate_population(population_size):\n",
        "\n",
        "  population = []\n",
        "  individual = []\n",
        "  conv_layers =  []\n",
        "  layer = []\n",
        "  dense_layers = []\n",
        "  for i in range(0,population_size):\n",
        "    \n",
        "    conv_layers =  []\n",
        "    nb_conv_layers = random.choice(nb_bloc_conv_valeus)\n",
        "    for j in range(nb_conv_layers):\n",
        "      layer.append(random.choice(nb_filters_valeus))\n",
        "      layer.append(random.choice(filter_size_valeus))\n",
        "      layer.append(random.choice(polling_size_valeus))\n",
        "      layer.append(random.choice([0,1])) #Choixe d'ajouter ou pas une couche d'activation\n",
        "      conv_layers.append(layer)\n",
        "      layer = []\n",
        "    individual.append(conv_layers)\n",
        "    \n",
        "    dense_layers = []\n",
        "    nb_dense_layers = random.choice(nb_dense_layer_valeus)\n",
        "    for j in range(nb_dense_layers):\n",
        "      layer.append(random.choice(nb_dense_units_valeus))\n",
        "      layer.append(random.choice([0,1])) #Choix d'ajouter ou pas une couche d'activation\n",
        "      dense_layers.append(layer)\n",
        "      layer=[]\n",
        "    individual.append(dense_layers)\n",
        "\n",
        "    population.append(individual)\n",
        "    individual = []\n",
        "\n",
        "  return population"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMOxQ6DeEZGT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijfNpLy5EbkS"
      },
      "outputs": [],
      "source": [
        "def fitness_function(individual,optimizer=None,input_shape=(),\n",
        "                     nb_classe=2,train_set = [],\n",
        "                     test_set=[],nb_epochs = 4, \n",
        "                     batch_size = 100,validation_split = 0.2):\n",
        "  try:\n",
        "    model = generat_model(optimizer=optimizer,input_shape=input_shape,nb_classe=nb_classe,individual=individual)\n",
        "    model.fit(x = train_set[0], batch_size=batch_size, epochs=nb_epochs)\n",
        "    # validation_split=validation_split,\n",
        "    test_loss, test_acc = model.evaluate(test_set[0], steps=len(test_set[0]))\n",
        "\n",
        "    print(f\"test loss:{test_loss}, test accuracy:{test_acc}\")\n",
        "  \n",
        "  except:\n",
        "    return 0\n",
        "    \n",
        "  return test_loss,test_acc\n",
        "\n",
        "def evaluate_population(population = [], optimizer = None,input_shape=(), train_set = [], test_set=[], nb_epochs = 15,batch_size = 50,file_path=None):\n",
        "  \n",
        "  evaluated_population = {}\n",
        "  evaluated = []\n",
        "  if len(train_set) != 0: \n",
        "    \n",
        "    for i,individual in enumerate(population):\n",
        "      print(\"Evaluation individu: \",i)\n",
        "      test_loss, fitness = fitness_function(optimizer=optimizer, individual = individual,input_shape=input_shape,\n",
        "                                 train_set=train_set,test_set=test_set,nb_epochs=nb_epochs,batch_size=batch_size)\n",
        "      #evaluated_population[tuple(individual)] = fitness\n",
        "      evaluated.append((individual,fitness))\n",
        "\n",
        "      with open(file_path,\"a\") as f:\n",
        "        f.write(f\"{individual}:{fitness} \\n\")\n",
        "      f.close()\n",
        "\n",
        "  return evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPO7NsIKxrA1"
      },
      "source": [
        "## Selection parents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tiq0L5qSXNg"
      },
      "outputs": [],
      "source": [
        "def best_ranked_selection(nb_parents, population:list):\n",
        "\n",
        "  population_sorted = sorted(population, key=lambda x: x[1],reverse=True)\n",
        "  parents  = [parent for parent in [indiv[0] for indiv in population_sorted]][0:nb_parents]\n",
        "  \n",
        "  return parents\n",
        "\n",
        "def rank_selection(population, num_parents):\n",
        "    fitness = [individual.fitness for individual in population]\n",
        "    rank = np.argsort(np.argsort(fitness))[::-1]\n",
        "    selected_parents = []\n",
        "    for i in range(num_parents):\n",
        "        rand = np.random.randint(0, sum(range(len(rank)))+1)\n",
        "        for j in range(len(rank)):\n",
        "            rand -= j\n",
        "            if rand < 0:\n",
        "                selected_parents.append(population[rank[j]])\n",
        "                break\n",
        "    return selected_parents\n",
        "\n",
        "def random_selection(population, nb_parents):\n",
        "  selected = []\n",
        "  parents = []\n",
        "  for i in range(nb_parents):\n",
        "    r = random.randint(0,len(population))\n",
        "    while r in selected:\n",
        "      r = random.randint(0,len(population))\n",
        "    \n",
        "    selected.append(r)\n",
        "    parents.append(list(population.keys())[r])\n",
        "  \n",
        "  return parents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ns6tbxVyBr2"
      },
      "source": [
        "## Corossover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep0SH6CGyMgy"
      },
      "outputs": [],
      "source": [
        "def Single_Point_Crossover(parent_1, parent_2):\n",
        "\n",
        "  child_1,child_2 = [],[]\n",
        "\n",
        "  min_length = min(len(parent_1[0]),len(parent_2[0]))\n",
        "\n",
        "  random_point = random.randint(0,min_length)\n",
        "\n",
        "  child_1.append(parent_1[0][0:random_point]+parent_2[0][random_point:len(parent_2[0])])\n",
        "  child_2.append(parent_2[0][0:random_point]+parent_1[0][random_point:len(parent_1[0])])\n",
        "\n",
        "  min_length = min(len(parent_1[1]),len(parent_2[1]))\n",
        "\n",
        "  random_point = random.randint(0,min_length)\n",
        "\n",
        "  child_1.append(parent_1[1][0:random_point]+parent_2[1][random_point:len(parent_2[1])])\n",
        "  child_2.append(parent_2[1][0:random_point]+parent_1[1][random_point:len(parent_1[1])])\n",
        "\n",
        "  return child_1,child_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJIF2zR_yT3k"
      },
      "source": [
        "## Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lloBa3byWNf"
      },
      "outputs": [],
      "source": [
        "def add_conv_layer(individual):\n",
        "  if len(individual[1])<max(nb_bloc_conv_valeus):\n",
        "    layer = []\n",
        "    layer.append(random.choice(nb_filters_valeus))\n",
        "    layer.append(random.choice(filter_size_valeus))\n",
        "    layer.append(random.choice(polling_size_valeus))\n",
        "    layer.append(random.choice([0,1]))\n",
        "\n",
        "    rand_pos = random.randint(0,len(individual[0]))\n",
        "\n",
        "    individual[0].insert(rand_pos,layer)\n",
        "  \n",
        "  return individual\n",
        "  \n",
        "\n",
        "def del_conv_layer(individual):\n",
        "  if len(individual[0]) > min(nb_bloc_conv_valeus):\n",
        "    rand_layer = random.randrange(0,len(individual[0]))\n",
        "    del individual[0][rand_layer]\n",
        "\n",
        "  return individual\n",
        " \n",
        "\n",
        "def alter_conv_layer(individual):\n",
        "\n",
        "  rand_layer = random.randrange(0,len(individual[0]))\n",
        "  rand_param = random.randrange(0,len(individual[0][0]))\n",
        "  val_selected = random.choice(search_space_conv[rand_param])\n",
        "\n",
        "  while val_selected == individual[0][rand_layer][rand_param]:\n",
        "    val_selected = random.choice(search_space_conv[rand_param])\n",
        "\n",
        "  individual[0][rand_layer][rand_param] = val_selected\n",
        "\n",
        "  return individual\n",
        "\n",
        "def add_dense_layer(individual):\n",
        "\n",
        "  if len(individual[1])<max(nb_dense_layer_valeus):\n",
        "    layer = []\n",
        "    \n",
        "    layer.append(random.choice(nb_dense_units_valeus))\n",
        "    layer.append(random.choice([0,1]))\n",
        "\n",
        "    rand_pos = random.randint(0,len(individual[1]))\n",
        "\n",
        "    individual[1].insert(rand_pos,layer)\n",
        "  \n",
        "  return individual\n",
        "  \n",
        "\n",
        "def del_dense_layer(individual):\n",
        "  if len(individual[1]) > min(nb_dense_layer_valeus):\n",
        "    rand_layer = random.randrange(0,len(individual[1]))\n",
        "    del individual[1][rand_layer]\n",
        "\n",
        "  return individual\n",
        "  \n",
        "\n",
        "def alter_dense_layer(individual):\n",
        "\n",
        "  rand_layer = random.randrange(0,len(individual[1]))\n",
        "  rand_param = random.randrange(0,len(individual[1][0]))\n",
        "  val_selected = random.choice(search_space_dense[rand_param])\n",
        "\n",
        "  while val_selected == individual[1][rand_layer][rand_param]:\n",
        "    val_selected = random.choice(search_space_dense[rand_param])\n",
        "\n",
        "  individual[1][rand_layer][rand_param] = val_selected\n",
        "\n",
        "  return individual\n",
        "  \n",
        "\n",
        "def uniform_mutation(individual):\n",
        "  \n",
        "  rand_operation = random.choice([1,2,3])\n",
        "\n",
        "  while rand_operation==2 and len(individual[1]) ==1:\n",
        "     rand_operation = random.choice([1,2,3])\n",
        "\n",
        "  if rand_operation == 1:individual = add_dense_layer(individual)\n",
        "  elif rand_operation == 2:individual = del_dense_layer(individual)\n",
        "  else:individual = alter_dense_layer(individual)\n",
        "\n",
        "  rand_operation = random.choice([1,2,3])\n",
        "\n",
        "  rand_operation = random.choice([1,2,3])\n",
        "  while rand_operation==2 and len(individual[1]) ==1:\n",
        "     rand_operation = random.choice([1,2,3])\n",
        "\n",
        "  if rand_operation == 1: individual = add_conv_layer(individual)\n",
        "  elif rand_operation == 2:individual = del_conv_layer(individual)\n",
        "  else: individual = alter_conv_layer(individual)\n",
        "  \n",
        "  return individual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mze_U3bfyhCV"
      },
      "source": [
        "## select next generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IzaFnFsypvg"
      },
      "outputs": [],
      "source": [
        "def select_next_generation(population = [],children=[],next_generation_size=10, elite_frac = 0.5, children_frac = 0.5):\n",
        "  \n",
        "  nb_population = int(next_generation_size*elite_frac)\n",
        "  nb_children = int(next_generation_size*children_frac)\n",
        "\n",
        "  population_sorted = sorted(population, key=lambda x: x[1], reverse=True)  \n",
        "\n",
        "  next_generation = []\n",
        "  next_generation +=  [indiv[0] for indiv in population_sorted[0:nb_population]]\n",
        "  next_generation += random.sample(children,nb_children)\n",
        "\n",
        "  if (elite_frac+children_frac) < 1 :\n",
        "    nb_random_indivs = int((1 - (elite_frac+children_frac)) * next_generation_size)\n",
        "\n",
        "    for i in range(nb_random_indivs):\n",
        "      random_number = random.randint(0, len(population))\n",
        "      next_generation.append(population.keys()[random_number])\n",
        "\n",
        "  return next_generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9l5YtoK_FaZ"
      },
      "source": [
        "## Select best in generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDXmXR6-_JtG"
      },
      "outputs": [],
      "source": [
        "def select_best_solution(population = []):\n",
        "  sorted_population = sorted(population, key=lambda x: x[1])\n",
        "  return sorted_population[-1] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR966f1gzdIu"
      },
      "source": [
        "## Algorithme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7jBvoBpL69-"
      },
      "outputs": [],
      "source": [
        "best_in_generation = []\n",
        "def Genetic_Algorithme(population_size, nb_generation,nb_parents,elite_frac,children_frac,optimizer,input_shape, train_set, test_set, nb_epochs,batch_size, file_path):\n",
        "  \n",
        "  #initalise la population aleatoirement\n",
        "  population = generate_population(population_size)\n",
        "  \n",
        "\n",
        "  for i in range(nb_generation):\n",
        "    \n",
        "    with open(file_path, \"a\") as f:\n",
        "      f.write(f\"Generation:{i} \\n\")\n",
        "    f.close()\n",
        "\n",
        "    print(\"Debut generation: \",i)\n",
        "    #Evaluer la population\n",
        "    population_evaluated = evaluate_population(population=population,optimizer=optimizer,input_shape=input_shape, train_set=train_set,test_set= test_set, \n",
        "                                     nb_epochs=nb_epochs,batch_size=batch_size, file_path=file_path)\n",
        "    \n",
        "    best_in_generation.append(select_best_solution(population_evaluated))\n",
        "    \n",
        "    #appliquer une selection en rouleutte pour\n",
        "    parents = best_ranked_selection(nb_parents,population_evaluated)\n",
        "    \n",
        "    #Appliquer le crossover pour cree de nouveau enfants\n",
        "    new_children = []\n",
        "    for parent_1, parent_2 in combinations(parents,2):\n",
        "      #appliquer le crossover sur chaque 2 pers de parents\n",
        "      child_1,child_2 = Single_Point_Crossover(parent_1,parent_2)\n",
        "      new_children.append(child_1)\n",
        "      new_children.append(child_2)\n",
        "\n",
        "    #Appliquer la mutation sur les enfants\n",
        "    children_after_mutation = []\n",
        "    for child in new_children:\n",
        "      mutated_child = uniform_mutation(child)\n",
        "      children_after_mutation.append(mutated_child)\n",
        "    \n",
        "    #selection la population de la future generation\n",
        "    population = select_next_generation(population_evaluated, children_after_mutation,population_size,elite_frac,children_frac)\n",
        "    \"\"\"\n",
        "    #Evaluer la population\n",
        "    population_evaluated = evaluate_population(population=population,optimizer=optimizer,input_shape=input_shape, train_set=train_set,test_set= test_set, \n",
        "                                     nb_epochs=nb_epochs,batch_size=batch_size,file_path)\"\"\"\n",
        "\n",
        "    \n",
        "  \n",
        "  \n",
        "  #Selectionner la meilleur solution attiente\n",
        "  best_solution = select_best_solution(population_evaluated)\n",
        "\n",
        "  return best_solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa88m5p3V4mk"
      },
      "source": [
        "#4. Datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdgpoCuec6w9",
        "outputId": "f9e81e48-b564-4744-a0f7-2e6cf5347a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60000 images belonging to 2 classes.\n",
            "Found 17121 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2)\n",
        "\n",
        "training_set= train_datagen.flow_from_directory(\n",
        "        '/content/Dataset/BaseDeDonneesV3_1/TrainSet',\n",
        "        target_size=(32, 32),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=150,\n",
        "        class_mode='categorical'\n",
        "        )\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "        rescale=1./255)\n",
        "\n",
        "test_set= test_datagen.flow_from_directory(\n",
        "        '/content/Dataset/BaseDeDonneesV3_1/TestSet',\n",
        "        target_size=(32, 32),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=150,\n",
        "        class_mode='categorical'\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NcA-6KYAAz6"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcUtdFbEFio6"
      },
      "outputs": [],
      "source": [
        "NB_OF_GENERATION = 10\n",
        "POPULATION_SIZE = 15\n",
        "NB_PARENTS = 10\n",
        "INPUT_SHAPE =(32,32,1)\n",
        "BATCH_SIZE = 100\n",
        "NB_EPOCHS = 8\n",
        "ELITE_FRAC = 0.5\n",
        "CHILDREN_FRAC = 0.5\n",
        "TEST_SIZE = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6FluhbGzbj_"
      },
      "outputs": [],
      "source": [
        "#train_setX, test_setX, train_setY, test_setY = load_dataset(\"/content/Dataset/train.csv\",TEST_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dg2_p1zH-hj"
      },
      "outputs": [],
      "source": [
        "date = datetime.datetime.now()\n",
        "FILE_NAME = f\"file_{date}.txt\"\n",
        "f = open(FILE_NAME,\"w\")\n",
        "f.write(f\"NB_OF_GENERATION = {NB_OF_GENERATION}\\nPOPULATION_SIZE = {POPULATION_SIZE}\\nINPUT_SHAPE = {INPUT_SHAPE}\\nBATCH_SIZE = {BATCH_SIZE}\\nNB_EPOCHS = {NB_EPOCHS}\\nELITE_FRAC = {ELITE_FRAC}\\nCHILDREN_FRAC = {CHILDREN_FRAC}\\nTEST_SIZE = {TEST_SIZE}\\n\")\n",
        "for best in best_in_generation: \n",
        "  print(best)\n",
        "  f.write(str(best)+\"\\n\")\n",
        "\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMGSYqR-ztch",
        "outputId": "acddcdb1-6050-460a-afa2-37d984569b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debut generation:  0\n",
            "Evaluation individu:  0\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 37s 68ms/step - loss: 0.6463 - accuracy: 0.6608\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 27s 66ms/step - loss: 0.5686 - accuracy: 0.6827\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5430 - accuracy: 0.7001\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 26s 64ms/step - loss: 0.5204 - accuracy: 0.7205\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.4859 - accuracy: 0.7557\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 26s 64ms/step - loss: 0.4515 - accuracy: 0.7854\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.4168 - accuracy: 0.8065\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.3834 - accuracy: 0.8280\n",
            "115/115 [==============================] - 7s 55ms/step - loss: 0.3432 - accuracy: 0.8532\n",
            "test loss:0.34320709109306335, test accuracy:0.8531627655029297\n",
            "Evaluation individu:  1\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 26s 62ms/step - loss: 0.6237 - accuracy: 0.6347\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.5514 - accuracy: 0.6942\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5334 - accuracy: 0.7064\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5210 - accuracy: 0.7187\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 25s 64ms/step - loss: 0.5148 - accuracy: 0.7231\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.5087 - accuracy: 0.7279\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.5031 - accuracy: 0.7324\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.4981 - accuracy: 0.7353\n",
            "115/115 [==============================] - 6s 54ms/step - loss: 0.4939 - accuracy: 0.7377\n",
            "test loss:0.49393659830093384, test accuracy:0.7376905679702759\n",
            "Evaluation individu:  2\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 1236s 3s/step - loss: 0.6322 - accuracy: 0.6291\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 1153s 3s/step - loss: 0.5397 - accuracy: 0.7053\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 1152s 3s/step - loss: 0.4782 - accuracy: 0.7602\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 1151s 3s/step - loss: 0.4087 - accuracy: 0.8123\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 1151s 3s/step - loss: 0.3320 - accuracy: 0.8585\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 1151s 3s/step - loss: 0.2770 - accuracy: 0.8878\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 1151s 3s/step - loss: 0.2355 - accuracy: 0.9066\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 1152s 3s/step - loss: 0.2050 - accuracy: 0.9203\n",
            "115/115 [==============================] - 41s 356ms/step - loss: 0.2174 - accuracy: 0.9184\n",
            "test loss:0.21744048595428467, test accuracy:0.9184042811393738\n",
            "Evaluation individu:  3\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 26s 62ms/step - loss: 0.6923 - accuracy: 0.5428\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 27s 66ms/step - loss: 0.6792 - accuracy: 0.6395\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.6007 - accuracy: 0.6729\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 27s 66ms/step - loss: 0.5546 - accuracy: 0.6922\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.5419 - accuracy: 0.6988\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 26s 64ms/step - loss: 0.5334 - accuracy: 0.7082\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.5257 - accuracy: 0.7128\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.5165 - accuracy: 0.7217\n",
            "115/115 [==============================] - 6s 53ms/step - loss: 0.5543 - accuracy: 0.6837\n",
            "test loss:0.5542938113212585, test accuracy:0.6837217211723328\n",
            "Evaluation individu:  4\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 45s 105ms/step - loss: 0.6268 - accuracy: 0.6399\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 43s 107ms/step - loss: 0.5609 - accuracy: 0.6862\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 43s 108ms/step - loss: 0.5437 - accuracy: 0.6976\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 43s 108ms/step - loss: 0.5287 - accuracy: 0.7063\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 44s 110ms/step - loss: 0.5101 - accuracy: 0.7239\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 43s 107ms/step - loss: 0.4901 - accuracy: 0.7448\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 43s 109ms/step - loss: 0.4604 - accuracy: 0.7691\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 44s 110ms/step - loss: 0.4210 - accuracy: 0.7986\n",
            "115/115 [==============================] - 8s 68ms/step - loss: 0.3915 - accuracy: 0.8164\n",
            "test loss:0.391491174697876, test accuracy:0.8163658380508423\n",
            "Evaluation individu:  5\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 30s 71ms/step - loss: 0.6841 - accuracy: 0.5891\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 28s 71ms/step - loss: 0.6138 - accuracy: 0.6544\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 28s 71ms/step - loss: 0.5668 - accuracy: 0.6837\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 29s 72ms/step - loss: 0.5440 - accuracy: 0.6971\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 30s 74ms/step - loss: 0.5230 - accuracy: 0.7147\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 29s 72ms/step - loss: 0.4989 - accuracy: 0.7422\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 29s 72ms/step - loss: 0.4666 - accuracy: 0.7708\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 28s 70ms/step - loss: 0.4328 - accuracy: 0.7959\n",
            "115/115 [==============================] - 7s 58ms/step - loss: 0.5955 - accuracy: 0.6730\n",
            "test loss:0.595483124256134, test accuracy:0.672974705696106\n",
            "Evaluation individu:  6\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 28s 67ms/step - loss: 0.5711 - accuracy: 0.6826\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.4990 - accuracy: 0.7446\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.4721 - accuracy: 0.7692\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.4528 - accuracy: 0.7822\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.4320 - accuracy: 0.7972\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 27s 68ms/step - loss: 0.4179 - accuracy: 0.8067\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 27s 67ms/step - loss: 0.4049 - accuracy: 0.8144\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.3904 - accuracy: 0.8263\n",
            "115/115 [==============================] - 7s 57ms/step - loss: 0.3765 - accuracy: 0.8348\n",
            "test loss:0.3764614760875702, test accuracy:0.834764301776886\n",
            "Evaluation individu:  7\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 27s 65ms/step - loss: 0.6923 - accuracy: 0.5261\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.6828 - accuracy: 0.6094\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5871 - accuracy: 0.6771\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5463 - accuracy: 0.6972\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5378 - accuracy: 0.7023\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 26s 66ms/step - loss: 0.5328 - accuracy: 0.7049\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5309 - accuracy: 0.7086\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 26s 65ms/step - loss: 0.5283 - accuracy: 0.7104\n",
            "115/115 [==============================] - 7s 60ms/step - loss: 0.5344 - accuracy: 0.7060\n",
            "test loss:0.5344204902648926, test accuracy:0.7060335278511047\n",
            "Evaluation individu:  8\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 30s 70ms/step - loss: 0.5942 - accuracy: 0.6637\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 28s 70ms/step - loss: 0.5352 - accuracy: 0.7114\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 28s 70ms/step - loss: 0.5023 - accuracy: 0.7418\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 28s 71ms/step - loss: 0.4960 - accuracy: 0.7495\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 28s 70ms/step - loss: 0.4719 - accuracy: 0.7655\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 28s 71ms/step - loss: 0.4624 - accuracy: 0.7742\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 28s 71ms/step - loss: 0.4480 - accuracy: 0.7832\n",
            "Epoch 8/8\n",
            " 97/400 [======>.......................] - ETA: 21s - loss: 0.4596 - accuracy: 0.7764"
          ]
        }
      ],
      "source": [
        "optimizer = SGD(0.02)\n",
        "best_solution = Genetic_Algorithme(POPULATION_SIZE,NB_OF_GENERATION,NB_PARENTS,ELITE_FRAC,CHILDREN_FRAC,optimizer,\n",
        "                  INPUT_SHAPE,[training_set],[test_set],NB_EPOCHS,BATCH_SIZE, FILE_NAME)\n",
        "print(\"Best Solution: \",best_solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA9LRgBygXU8"
      },
      "outputs": [],
      "source": [
        "!cp -r \"file_{date}.txt\" \"/content/drive/MyDrive/Colab Notebooks/Projet Master/Tests/file_{date}.txt\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}