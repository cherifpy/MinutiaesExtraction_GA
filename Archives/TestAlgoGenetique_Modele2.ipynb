{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Algo genetique avec:\n",
        "- un crossover sur la 1 et 2 parties suelement\n",
        "- Crossover uniform \n",
        "- reduire l'espace de recherche\n",
        "- Encodage Statique\n"
      ],
      "metadata": {
        "id": "HDMCE6Libexc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qxfgINihUfV"
      },
      "source": [
        "#1. Importer DataSet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY8LvMn0hSL_",
        "outputId": "523714ad-d52e-4ac8-8c9a-35fc24acc0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8fm0P7MlhdGG"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Projet Master/DataSets/BaseDeDonnees_2.zip\" minutiae_detection.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO9lYiyOG1gQ",
        "outputId": "465704f3-81fc-4c8d-9657-a2296115a64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deecompression .....\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "print(\"Deecompression .....\")\n",
        "with ZipFile('minutiae_detection.zip') as zip:\n",
        "    zip.extractall('Dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB9nxXShjN6a"
      },
      "source": [
        "#2. Loads libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DF10g-TDjRE6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Conv2D, Dense, Input, MaxPool2D, Dropout, Activation, Normalization, Flatten, Reshape\n",
        "from keras.activations import relu, softmax\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import random \n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import csv\n",
        "import cv2 as cv\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgP49YVqjj8h"
      },
      "source": [
        "#3. Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "guHxYoZ0KG-c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def CreateModel1(optimizer=None,input_shape=(),nb_classe=2, individual=[],\n",
        "                    version=\"static\",loss='categorical_crossentropy',\n",
        "                    mertics=\"accuracy\"):\n",
        "    \"\"\"\n",
        "        Fonction de ceation du pemiere modele utilisé pour la classification des blocs\n",
        "            -  Input Shape : dimension d'entré\n",
        "            -  nb_classe: nombre de classe de classification dans notre cas c'est 2\n",
        "            -  individual: le chromosome de l'individu\n",
        "            -  version: version de l'encodage soit \"dynamique\" ou \"statique\"\n",
        "            -  loss: la fonction erreur\n",
        "            -  metrics: metrique d'evaluation\n",
        "    \"\"\"\n",
        "    my_model = Sequential(name=\"Model\")\n",
        "    my_model.add(Input(shape=input_shape))\n",
        "    #my_model.add(Reshape((28,28,1)))\n",
        "\n",
        "    conv_layers = individual[0]\n",
        "    dense_layers = individual[1]\n",
        "\n",
        "    if version==\"dynamic\":\n",
        "        for i,layer in enumerate(conv_layers):\n",
        "            my_model.add(Conv2D(layer[0], layer[1], name=f'conv{i}',padding=\"same\")) #Add conv parameters\n",
        "            if layer[3] == 1: my_model.add(Activation(relu))\n",
        "            if layer[2] != 0: my_model.add(MaxPool2D((layer[2],layer[2]),padding=\"same\"))\n",
        "\n",
        "        my_model.add(Flatten())\n",
        "\n",
        "        for i,layer in enumerate(dense_layers):\n",
        "            my_model.add(Dense(layer[0]))\n",
        "            if layer[1] == 1:  my_model.add(Activation(relu))\n",
        "\n",
        "    elif version==\"static\":\n",
        "        for i,layer in enumerate(conv_layers):\n",
        "            if layer != []:\n",
        "                my_model.add(Conv2D(layer[0], layer[1], name=f'conv{i}',padding=\"same\")) #Add conv parameters\n",
        "                if layer[3] == 1:  my_model.add(Activation(relu))\n",
        "                if layer[2] != 0: my_model.add(MaxPool2D((layer[2],layer[2]),padding=\"same\"))\n",
        "    \n",
        "        my_model.add(Flatten())\n",
        "\n",
        "        for i,layer in enumerate(dense_layers):\n",
        "            if layer != []:\n",
        "                my_model.add(Dense(layer[0]))\n",
        "                if layer[1] == 1:  my_model.add(Activation(relu))\n",
        "    else:\n",
        "        print(\"Erreur de version dans le parametre version\")\n",
        "    \n",
        "    my_model.add(Dense(nb_classe,activation=softmax))\n",
        "\n",
        "    my_model.compile(loss=loss, optimizer=optimizer, metrics=[mertics])\n",
        "\n",
        "    return my_model\n",
        "\n",
        "\n",
        "def LossFunction(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "    \n",
        "def CreateModel2(optimizer=None,input_shape=(),nb_features=2, individual=[],\n",
        "                version=\"statique\",mertics=\"accuracy\"):\n",
        "    \"\"\"\n",
        "        Fonction de ceation du 2eme modele utilisé detecter la minuties dnas les blocs\n",
        "            -  Input Shape : dimension d'entré\n",
        "            -  nb_feature: nombre de parametre predire\n",
        "            -  individual: le chromosome de l'individu\n",
        "            -  version: version de l'encodage soit \"dynamique\" ou \"statique\"\n",
        "            -  loss: la fonction erreur\n",
        "            -  metrics: metrique d'evaluation\n",
        "    \"\"\"\n",
        "    my_model = Sequential(name=\"Model\")\n",
        "    my_model.add(Input(shape=input_shape))\n",
        "    #my_model.add(Reshape((28,28,1)))\n",
        "\n",
        "    conv_layers = individual[0]\n",
        "    dense_layers = individual[1]\n",
        "\n",
        "    if version==\"dynamique\":\n",
        "        for i,layer in enumerate(conv_layers):\n",
        "            my_model.add(Conv2D(layer[0], layer[1], name=f'conv{i}',padding=\"same\")) #Add conv parameters\n",
        "            if layer[3] == 1: my_model.add(Activation(relu))\n",
        "            if layer[2] != 0: my_model.add(MaxPool2D((layer[2],layer[2]),padding=\"same\"))\n",
        "\n",
        "        my_model.add(Flatten())\n",
        "\n",
        "        for i,layer in enumerate(dense_layers):\n",
        "            my_model.add(Dense(layer[0]))\n",
        "            if layer[1] == 1:  my_model.add(Activation(relu))\n",
        "\n",
        "    elif version==\"statique\":\n",
        "        for i,layer in enumerate(conv_layers):\n",
        "            if layer != []:\n",
        "                my_model.add(Conv2D(layer[0], layer[1], name=f'conv{i}',padding=\"same\")) #Add conv parameters\n",
        "                if layer[3] == 1:  my_model.add(Activation(relu))\n",
        "                if layer[2] != 0: my_model.add(MaxPool2D((layer[2],layer[2]),padding=\"same\"))\n",
        "    \n",
        "        my_model.add(Flatten())\n",
        "\n",
        "        for i,layer in enumerate(dense_layers):\n",
        "            if layer != []:\n",
        "                my_model.add(Dense(layer[0]))\n",
        "                if layer[1] == 1:  my_model.add(Activation(relu))\n",
        "    else:\n",
        "        print(\"Erreur de version dans le parametre version\")\n",
        "    \n",
        "    my_model.add(Dense(nb_features))\n",
        "    my_model.compile(loss=LossFunction, optimizer=optimizer, metrics=[mertics])\n",
        "\n",
        "    return my_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMvI7eySTF3"
      },
      "source": [
        "#4. Genetic Algorithme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XKPbuS6Beax"
      },
      "source": [
        "## Search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VuUE9B7TK2fT"
      },
      "outputs": [],
      "source": [
        "nb_bloc_conv_valeus = [5,6,7,8]\n",
        "nb_filters_valeus = [32,64,128,256] #[10,20,30,40]\n",
        "filter_size_valeus = [3,5,7,9]\n",
        "nb_dense_layer_valeus = [1,2,3]\n",
        "nb_dense_units_valeus = [64,128,256,512]\n",
        "polling_size_valeus = [0,3,5] #Dans le cas ou c'est 0 on ajoute pas de pooling\n",
        "\n",
        "learning_rate = [0.1,0.01,0.02,0.001,0.002,0.0001] \n",
        "\n",
        "search_space_conv = [nb_filters_valeus,filter_size_valeus,polling_size_valeus,[0,1]]\n",
        "search_space_dense = [nb_dense_units_valeus,[0,1]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6mQIRyoxR27"
      },
      "source": [
        "## Population generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T_vZR3UBxX8D"
      },
      "outputs": [],
      "source": [
        "def InitPopulation(population_size,version=\"static\"):\n",
        "\n",
        "    population = []\n",
        "    individual = []\n",
        "    conv_layers =  []\n",
        "    layer = []\n",
        "    dense_layers = []\n",
        "\n",
        "    for i in range(0,population_size):\n",
        "    \n",
        "        conv_layers =  []\n",
        "        nb_conv_layers = random.choice(nb_bloc_conv_valeus)\n",
        "        for j in range(nb_conv_layers):\n",
        "            layer.append(random.choice(nb_filters_valeus))\n",
        "            layer.append(random.choice(filter_size_valeus))\n",
        "            layer.append(random.choice(polling_size_valeus))\n",
        "            layer.append(random.choice([0,1])) #Choixe d'ajouter ou pas une couche d'activation\n",
        "            conv_layers.append(layer)\n",
        "            layer = []\n",
        "        \n",
        "        if version == \"static\":\n",
        "            for i in range(nb_conv_layers, max(nb_bloc_conv_valeus)):\n",
        "                conv_layers.append([])\n",
        "        \n",
        "        individual.append(conv_layers)\n",
        "        \n",
        "        dense_layers = []\n",
        "        nb_dense_layers = random.choice(nb_dense_layer_valeus)\n",
        "        for j in range(nb_dense_layers):\n",
        "            layer.append(random.choice(nb_dense_units_valeus))\n",
        "            layer.append(random.choice([0,1])) #Choix d'ajouter ou pas une couche d'activation\n",
        "            dense_layers.append(layer)\n",
        "            layer=[]\n",
        "        \n",
        "        if version ==\"static\":\n",
        "            for i in range(nb_dense_layers, max(nb_dense_layer_valeus)):\n",
        "                dense_layers.append([])\n",
        "        individual.append(dense_layers)\n",
        "\n",
        "        population.append(individual)\n",
        "        individual = []\n",
        "\n",
        "    return population"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMOxQ6DeEZGT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ijfNpLy5EbkS"
      },
      "outputs": [],
      "source": [
        "def Fitness(individual,optimizer=None,input_shape=(),\n",
        "                     nb_classe=2,train_set = [],\n",
        "                     test_set=[],nb_epochs = 4, \n",
        "                     batch_size = 100,validation_split = 0.2):\n",
        "    #try:\n",
        "    if len(train_set) == 1:\n",
        "        model = CreateModel1(optimizer=optimizer,input_shape=input_shape,nb_classe=nb_classe,individual=individual)\n",
        "        model.fit(x = train_set[0], batch_size=batch_size, epochs=nb_epochs)\n",
        "        # validation_split=validation_split,\n",
        "        test_loss, test_acc = model.evaluate(test_set[0], steps=len(test_set[0]))\n",
        "        #print(f\"test loss:{test_loss}, test accuracy:{test_acc}\")\n",
        "    else:\n",
        "        model = CreateModel2(optimizer=optimizer,input_shape=input_shape,nb_features=nb_classe,individual=individual)\n",
        "        model.fit(x = train_set[0],y = train_set[1],batch_size=batch_size, epochs=nb_epochs)\n",
        "        # validation_split=validation_split,\n",
        "        test_loss, test_acc = model.evaluate(test_set[0],test_set[1], steps=len(test_set[0]))\n",
        "\n",
        "    #except:\n",
        "    #    print(\"Oups!\")\n",
        "    #    return 0,0\n",
        "    \n",
        "    return test_loss,test_acc\n",
        "\n",
        "def EvaluatePopulation(population = [], optimizer = None,input_shape=(),\n",
        "                        train_set = [], test_set=[], nb_epochs = 15,\n",
        "                        batch_size = 50,file_path1=None,file_path2=None):\n",
        "\n",
        "    evaluation = []\n",
        "    if len(train_set) != 0: \n",
        "        \n",
        "        for i,individual in enumerate(population):\n",
        "            print(\"Evaluation individu: \",i)\n",
        "            debut = time.time()\n",
        "            test_loss, fitness = Fitness(optimizer=optimizer, individual = individual,input_shape=input_shape,\n",
        "                                    train_set=train_set,test_set=test_set,nb_epochs=nb_epochs,batch_size=batch_size)\n",
        "            #evaluated_population[tuple(individual)] = fitness\n",
        "            fin = time.time()\n",
        "            \n",
        "            data = {\"test accuracy\":fitness,\"time\":fin-debut}\n",
        "            WriteOnCSV(file_path2,data)\n",
        "\n",
        "            evaluation.append((individual,fitness))\n",
        "\n",
        "            with open(file_path1,\"a\") as f:\n",
        "                f.write(f\"{individual}\\n\")\n",
        "            f.close()\n",
        "\n",
        "    return evaluation\n",
        "\n",
        "def WriteOnCSV(file_path, data):\n",
        "    file = open(file_path, \"a\",newline='')\n",
        "    writer = csv.DictWriter(file, fieldnames=list(data.keys()))\n",
        "    writer.writerow(data)\n",
        "    file.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPO7NsIKxrA1"
      },
      "source": [
        "## Selection parents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4Tiq0L5qSXNg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def SelectBestSolution(population = []):\n",
        "    sorted_population = sorted(population, key=lambda x: x[1])\n",
        "    return sorted_population[-1]\n",
        "\n",
        "\n",
        "def BestRankedSelection(nb_parents, population:list):\n",
        "\n",
        "    population_sorted = sorted(population, key=lambda x: x[1],reverse=True)\n",
        "    parents  = [parent[0] for parent in population_sorted][0:nb_parents]\n",
        "    \n",
        "    return parents\n",
        "\n",
        "def rank_selection(population, num_parents):\n",
        "    fitness = [individual.fitness for individual in population]\n",
        "    rank = np.argsort(np.argsort(fitness))[::-1]\n",
        "    selected_parents = []\n",
        "    for i in range(num_parents):\n",
        "        rand = np.random.randint(0, sum(range(len(rank)))+1)\n",
        "        for j in range(len(rank)):\n",
        "            rand -= j\n",
        "            if rand < 0:\n",
        "                selected_parents.append(population[rank[j]])\n",
        "                break\n",
        "    return selected_parents\n",
        "\n",
        "def random_selection(population, nb_parents):\n",
        "    selected = []\n",
        "    parents = []\n",
        "    for i in range(nb_parents):\n",
        "        r = random.randint(0,len(population))\n",
        "        while r in selected:\n",
        "            r = random.randint(0,len(population))\n",
        "    \n",
        "        selected.append(r)\n",
        "        parents.append(list(population.keys())[r])\n",
        "  \n",
        "    return parents\n",
        "\n",
        "\n",
        "def SelectionAphaBeta(population,children, alpha, beta):\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ns6tbxVyBr2"
      },
      "source": [
        "## Corossover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ep0SH6CGyMgy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def CrossoverConv(parent_1, parent_2, proba_crossover = 0.7):\n",
        "    \"\"\"\n",
        "        Crossover applique sur la partie des couches convolution\n",
        "            -  proba_crossover: la probabilité de faire un crossover\n",
        "    \"\"\"\n",
        "    if random.random() < proba_crossover:\n",
        "        child_1,child_2 = [],[]\n",
        "        min_length = min(len(parent_1[0]),len(parent_2[0]))\n",
        "        random_point = random.randint(0,min_length)\n",
        "        child_1.append(parent_1[0][0:random_point]+parent_2[0][random_point:len(parent_2[0])])\n",
        "        child_2.append(parent_2[0][0:random_point]+parent_1[0][random_point:len(parent_1[0])])\n",
        "        min_length = min(len(parent_1[1]),len(parent_2[1]))\n",
        "        random_point = random.randint(0,min_length)\n",
        "        child_1.append(parent_1[1])\n",
        "        child_2.append(parent_2[1])\n",
        "\n",
        "        return child_1,child_2\n",
        "    else:\n",
        "        return parent_1, parent_2\n",
        "\n",
        "def CrossoverDense(parent_1, parent_2, proba_crossover=0.7):\n",
        "    \"\"\"\n",
        "        Crossover applique sur la partie des couches fully connected\n",
        "            -  proba_crossover: la probabilité de faire un crossover\n",
        "    \"\"\"\n",
        "    if random.random() < proba_crossover:\n",
        "        child_1,child_2 = [],[]\n",
        "        min_length = min(len(parent_1[0]),len(parent_2[0]))\n",
        "        random_point = random.randint(0,min_length)\n",
        "        child_1.append(parent_1[1][0:random_point]+parent_2[1][random_point:len(parent_2[0])])\n",
        "        child_2.append(parent_2[1][0:random_point]+parent_1[1][random_point:len(parent_1[0])])\n",
        "        min_length = min(len(parent_1[1]),len(parent_2[1]))\n",
        "        random_point = random.randint(0,min_length)\n",
        "        child_1.append(parent_1[0])\n",
        "        child_2.append(parent_2[0])\n",
        "\n",
        "        return child_1,child_2\n",
        "    else:\n",
        "        return parent_1, parent_2\n",
        "\n",
        "def Crossover2Parties(parent_1, parent_2, proba_crossover):\n",
        "    \"\"\"\n",
        "        Crossover applique sur les deux parties du chromosome\n",
        "            -  proba_crossover: la probabilité de faire un crossover\n",
        "    \"\"\"\n",
        "    if random.random() < proba_crossover:\n",
        "        child_1,child_2 = [],[]\n",
        "        min_length = min(len(parent_1[0]),len(parent_2[0]))\n",
        "        random_point = random.randint(0,min_length)\n",
        "        child_1.append(parent_1[0][0:random_point]+parent_2[0][random_point:len(parent_2[0])])\n",
        "        child_2.append(parent_2[0][0:random_point]+parent_1[0][random_point:len(parent_1[0])])\n",
        "        min_length = min(len(parent_1[1]),len(parent_2[1]))\n",
        "        random_point = random.randint(0,min_length)\n",
        "        child_1.append(parent_1[1][0:random_point]+parent_2[1][random_point:len(parent_2[1])])\n",
        "        child_2.append(parent_2[1][0:random_point]+parent_1[1][random_point:len(parent_1[1])])\n",
        "\n",
        "        return child_1,child_2\n",
        "    else:\n",
        "        return parent_1, parent_2\n",
        "\n",
        "\n",
        "def UniformCrossover(parent_1,parent_2,proba_crossover):\n",
        "    \"\"\"\n",
        "        Crossover uniforme applique sur les deux parties du chromosome \n",
        "            -  proba_crossover: la probabilité de faire un crossover\n",
        "    \"\"\"\n",
        "    child_1, child_2 = [[],[]],[[],[]]\n",
        "\n",
        "    for i in range(len(parent_1[0])):\n",
        "        if random.random() < proba_crossover: \n",
        "            child_1[0][i] = parent_1[0][i]\n",
        "            child_2[0][i] = parent_2[0][i]\n",
        "        else : \n",
        "            child_1[0][i] = parent_2[0][i]\n",
        "            child_2[0][i] = parent_1[0][i]\n",
        "\n",
        "    for i in range(len(parent_1[1])):\n",
        "        if random.random() < proba_crossover: \n",
        "            child_1[1][i] = parent_1[1][i]\n",
        "            child_2[1][i] = parent_2[1][i]\n",
        "        else : \n",
        "            child_1[1][i] = parent_2[1][i]\n",
        "            child_2[1][i] = parent_1[1][i]\n",
        "\n",
        "    return child_1,child_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJIF2zR_yT3k"
      },
      "source": [
        "## Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6lloBa3byWNf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def AddConvLayer(individual,i):\n",
        "    \n",
        "    layer = []\n",
        "    layer.append(random.choice(nb_filters_valeus))\n",
        "    layer.append(random.choice(filter_size_valeus))\n",
        "    layer.append(random.choice(polling_size_valeus))\n",
        "    layer.append(random.choice([0,1]))\n",
        "\n",
        "    individual[0][i] = layer\n",
        "    \n",
        "    return individual\n",
        "  \n",
        "\n",
        "def DelConvLayer(individual,i):\n",
        "\n",
        "    individual[0][i] = []\n",
        "    return individual\n",
        " \n",
        "\n",
        "def AlterConvLayer(individual,i):\n",
        "\n",
        "    rand_param = random.randrange(0,len(individual[0][i]))\n",
        "    val_selected = random.choice(search_space_conv[rand_param])\n",
        "    while val_selected == individual[0][i][rand_param]:\n",
        "        val_selected = random.choice(search_space_conv[rand_param])\n",
        "    individual[0][i][rand_param] = val_selected\n",
        "    return individual\n",
        "\n",
        "def AddDenseLayer(individual,i):\n",
        "\n",
        "    layer = []\n",
        "    \n",
        "    layer.append(random.choice(nb_dense_units_valeus))\n",
        "    layer.append(random.choice([0,1]))\n",
        "\n",
        "    individual[1][i] = layer\n",
        "    \n",
        "    return individual\n",
        "  \n",
        "\n",
        "def DelDenseLayer(individual,i):\n",
        "    individual[1][i] = []\n",
        "    return individual\n",
        "  \n",
        "\n",
        "def AlterDenseLayer(individual,i):\n",
        "\n",
        "    rand_param = random.randrange(0,len(individual[1][i]))\n",
        "    val_selected = random.choice(search_space_dense[rand_param])\n",
        "    while val_selected == individual[1][i][rand_param]:\n",
        "        val_selected = random.choice(search_space_dense[rand_param])\n",
        "    individual[1][i][rand_param] = val_selected\n",
        "\n",
        "    return individual\n",
        "\n",
        "def Mutation(individual, proba_mutation=1):\n",
        "    count1, count2 = 0,0\n",
        "    del_layer_conv, del_layer_dense = True,True\n",
        "\n",
        "    for layer in individual[0]: \n",
        "        if layer == []: count1 += 1\n",
        "    if count1<= min(nb_bloc_conv_valeus): del_layer_conv = False\n",
        "\n",
        "    for layer in individual[1]:\n",
        "            if layer == []:count2 += 1\n",
        "    if count2<= min(nb_bloc_conv_valeus): del_layer_dense = False\n",
        "        \n",
        "\n",
        "    for i, layer in enumerate(individual[0]):\n",
        "        if random.random() < proba_mutation:\n",
        "\n",
        "            if layer != []: \n",
        "                r = random.choice([1,2])\n",
        "                if r == 1 : individual = AlterConvLayer(individual, i)\n",
        "                elif del_layer_conv : individual = DelConvLayer(individual,i)\n",
        "                else : individual = AlterConvLayer(individual, i)\n",
        "\n",
        "            else : individual = AddConvLayer(individual,i)\n",
        "        \n",
        "    for i, layer in enumerate(individual[1]):\n",
        "        if random.random() < proba_mutation:\n",
        "\n",
        "            if layer != []: \n",
        "                r = random.choice([1,2])\n",
        "                if r == 1 : individual = AlterDenseLayer(individual, i)\n",
        "                elif del_layer_dense : individual = DelDenseLayer(individual,i)\n",
        "                else : individual = AlterDenseLayer(individual, i)\n",
        "\n",
        "            else: individual = AddDenseLayer(individual,i)\n",
        "    \n",
        "    return individual\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mze_U3bfyhCV"
      },
      "source": [
        "## select next generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6IzaFnFsypvg"
      },
      "outputs": [],
      "source": [
        "def SelectNextGeneration(population = [],children=[],generation_size=10, elite_frac = 0.5, children_frac = 0.5):\n",
        "  \n",
        "    nb_population = int(generation_size*elite_frac)\n",
        "    nb_children = int(generation_size*children_frac)\n",
        "\n",
        "    population_sorted = sorted(population, key=lambda x: x[1], reverse=True)  \n",
        "\n",
        "    next_generation = []\n",
        "    next_generation +=  [indiv[0] for indiv in population_sorted[0:nb_population]]\n",
        "    next_generation += random.sample(children,nb_children)\n",
        "\n",
        "    if (nb_population+nb_children) < generation_size :\n",
        "        nb_random_indivs = int((1 - (elite_frac+children_frac)) * generation_size)\n",
        "\n",
        "        for i in range(nb_random_indivs):\n",
        "            random_number = random.sample(list(population.keys()),nb_random_indivs)\n",
        "\n",
        "    return next_generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR966f1gzdIu"
      },
      "source": [
        "## Algorithme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I7jBvoBpL69-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def GeneticAlgorithme(population_size, nb_generation,nb_parents,elite_frac,\n",
        "                      children_frac,optimizer,input_shape, train_set, test_set, \n",
        "                      nb_epochs,batch_size, file_path1,file_path2, proba_crossover,proba_mutation ):\n",
        "    best_in_generation = []\n",
        "    #initalise la population aleatoirement\n",
        "    population = InitPopulation(population_size)\n",
        "    \n",
        "    population_evaluated = EvaluatePopulation(population=population,optimizer=optimizer,input_shape=input_shape, train_set=train_set,test_set= test_set, \n",
        "                                        nb_epochs=nb_epochs,batch_size=batch_size, file_path1=file_path1,file_path2=file_path2)\n",
        "    best_in_generation.append(SelectBestSolution(population_evaluated))\n",
        "\n",
        "    for i in range(nb_generation):\n",
        "    \n",
        "        with open(file_path1, \"a\") as f:\n",
        "            f.write(f\"Generation:{i} \\n\")\n",
        "        f.close()\n",
        "\n",
        "        print(\"Debut generation: \",i)\n",
        "        #Evaluer la population\n",
        "        \"\"\"\n",
        "        population_evaluated = EvaluatePopulation(population=population,optimizer=optimizer,input_shape=input_shape, train_set=train_set,test_set= test_set, \n",
        "                                        nb_epochs=nb_epochs,batch_size=batch_size, file_path1=file_path1,file_path2=file_path2)\n",
        "        \"\"\"\n",
        "        \n",
        "        parents = BestRankedSelection(nb_parents,population_evaluated)\n",
        "        \n",
        "        #Appliquer le crossover pour cree de nouveau enfants\n",
        "        new_children = []\n",
        "        for parent_1, parent_2 in combinations(parents,2):\n",
        "            #appliquer le crossover sur chaque 2 pers de parents\n",
        "            child_1,child_2 = CrossoverConv(parent_1,parent_2,proba_crossover)\n",
        "            new_children.append(child_1)\n",
        "            new_children.append(child_2)\n",
        "\n",
        "        #Appliquer la mutation sur les enfants\n",
        "        children_after_mutation = []\n",
        "        for child in new_children:\n",
        "            mutated_child = Mutation(child,proba_mutation)\n",
        "            children_after_mutation.append(mutated_child)\n",
        "        \n",
        "        #selection la population de la future generation\n",
        "        population = SelectNextGeneration(population_evaluated, children_after_mutation,population_size,elite_frac,children_frac)\n",
        "        \n",
        "        \n",
        "        #Evaluer la population\n",
        "        population_evaluated = EvaluatePopulation(population=population,optimizer=optimizer,input_shape=input_shape, train_set=train_set,test_set= test_set, \n",
        "                                        nb_epochs=nb_epochs,batch_size=batch_size, file_path1=file_path1,file_path2=file_path2)\n",
        "        best_in_generation.append(SelectBestSolution(population_evaluated))\n",
        "    \n",
        "    #Selectionner la meilleur solution attiente\n",
        "    #best_in_generation = SelectBestSolution(population_evaluated, best_in_generation)\n",
        "\n",
        "    return best_in_generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa88m5p3V4mk"
      },
      "source": [
        "#4. Datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bdgpoCuec6w9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def LoadDataBase1(TrainingPath:str, TestPath:str,batchsize=150):\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2)\n",
        "\n",
        "    training_set= train_datagen.flow_from_directory(\n",
        "        TrainingPath,\n",
        "        target_size=(32, 32),\n",
        "        color_mode=\"grayscale\",\n",
        "        batch_size=batchsize,\n",
        "        class_mode='categorical'\n",
        "        )\n",
        "    \n",
        "    \n",
        "    test_datagen = ImageDataGenerator(\n",
        "            rescale=1./255)\n",
        "\n",
        "    test_set= test_datagen.flow_from_directory(\n",
        "            TestPath,\n",
        "            target_size=(32, 32),\n",
        "            color_mode=\"grayscale\",\n",
        "            batch_size=batchsize,\n",
        "            class_mode='categorical'\n",
        "            )\n",
        "    \n",
        "    return training_set,test_set\n",
        "\n",
        "def LoadDataBase2(DataSet_Path:str,Images_Path:str, TestSplit=0.2):\n",
        "    \n",
        "    TrainSet_X, TestSet_X= [],[]\n",
        "    df = pd.read_csv(DataSet_Path)\n",
        "    Train, Test = train_test_split(df, test_size=TestSplit,shuffle=True)\n",
        "\n",
        "\n",
        "    for path in Train[\"Images\"]:\n",
        "      img = cv.imread(Images_Path+\"/\"+path, cv.IMREAD_GRAYSCALE)\n",
        "      #img = np.reshape(img,(32,32,1))\n",
        "\n",
        "      TrainSet_X.append(img)\n",
        "\n",
        "    for path in Test[\"Images\"]:\n",
        "      img = cv.imread(Images_Path+\"/\"+path, cv.IMREAD_GRAYSCALE)\n",
        "      #img = np.reshape(img,(32,32,1))\n",
        "\n",
        "      TestSet_X.append(img)\n",
        "\n",
        "    TrainSet_Y = Train[[\"X\",\"Y\"]]\n",
        "    TestSet_Y = Test[[\"X\",\"Y\"]]\n",
        "\n",
        "    TrainSet_X = np.array(TrainSet_X)\n",
        "    TestSet_X = np.array(TestSet_X)\n",
        "    print(f\"Taille de Train {len(TrainSet_X)} {len(TrainSet_Y)}\")\n",
        "    print(f\"Taille de Test {len(TestSet_X)} {len(TestSet_Y)}\")\n",
        "\n",
        "    return TrainSet_X,TestSet_X,TrainSet_Y, TestSet_Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NcA-6KYAAz6"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dg2_p1zH-hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e22243-76b1-4d5b-9719-175657814a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille de Train 62412 62412\n",
            "Taille de Test 26748 26748\n",
            "Evaluation individu:  0\n",
            "Epoch 1/15\n",
            "417/417 [==============================] - 10s 21ms/step - loss: 249.0287 - accuracy: 0.5224\n",
            "Epoch 2/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5252 - accuracy: 0.5224\n",
            "Epoch 3/15\n",
            "417/417 [==============================] - 9s 20ms/step - loss: 249.5253 - accuracy: 0.5224\n",
            "Epoch 4/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5252 - accuracy: 0.5224\n",
            "Epoch 5/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5254 - accuracy: 0.5224\n",
            "Epoch 6/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5253 - accuracy: 0.5224\n",
            "Epoch 7/15\n",
            "417/417 [==============================] - 8s 19ms/step - loss: 249.5253 - accuracy: 0.5224\n",
            "Epoch 8/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5251 - accuracy: 0.5224\n",
            "Epoch 9/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5253 - accuracy: 0.5224\n",
            "Epoch 10/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5252 - accuracy: 0.5224\n",
            "Epoch 11/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5253 - accuracy: 0.5224\n",
            "Epoch 12/15\n",
            "417/417 [==============================] - 8s 19ms/step - loss: 249.5251 - accuracy: 0.5224\n",
            "Epoch 13/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5251 - accuracy: 0.5224\n",
            "Epoch 14/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5253 - accuracy: 0.5224\n",
            "Epoch 15/15\n",
            "417/417 [==============================] - 8s 20ms/step - loss: 249.5251 - accuracy: 0.5224\n",
            "26748/26748 [==============================] - 96s 4ms/step - loss: 249.5155 - accuracy: 0.5227\n",
            "Evaluation individu:  1\n",
            "Epoch 1/15\n",
            "417/417 [==============================] - 122s 270ms/step - loss: 36.9783 - accuracy: 0.4776\n",
            "Epoch 2/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 27.4304 - accuracy: 0.4776\n",
            "Epoch 3/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 25.2401 - accuracy: 0.4776\n",
            "Epoch 4/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 24.0229 - accuracy: 0.4776\n",
            "Epoch 5/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 23.2312 - accuracy: 0.4776\n",
            "Epoch 6/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 22.6987 - accuracy: 0.4776\n",
            "Epoch 7/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 22.3147 - accuracy: 0.4776\n",
            "Epoch 8/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 22.0335 - accuracy: 0.4776\n",
            "Epoch 9/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.8284 - accuracy: 0.4776\n",
            "Epoch 10/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.6922 - accuracy: 0.4776\n",
            "Epoch 11/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.5995 - accuracy: 0.4776\n",
            "Epoch 12/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.5364 - accuracy: 0.4776\n",
            "Epoch 13/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.4992 - accuracy: 0.4776\n",
            "Epoch 14/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.4815 - accuracy: 0.4776\n",
            "Epoch 15/15\n",
            "417/417 [==============================] - 110s 263ms/step - loss: 21.4748 - accuracy: 0.4776\n",
            "26748/26748 [==============================] - 97s 4ms/step - loss: 21.5001 - accuracy: 0.4773\n",
            "Evaluation individu:  2\n",
            "Epoch 1/15\n",
            "417/417 [==============================] - 5s 9ms/step - loss: 249.5663 - accuracy: 0.4776\n",
            "Epoch 2/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7622 - accuracy: 0.4776\n",
            "Epoch 3/15\n",
            "417/417 [==============================] - 4s 8ms/step - loss: 249.7623 - accuracy: 0.4776\n",
            "Epoch 4/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7623 - accuracy: 0.4776\n",
            "Epoch 5/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7623 - accuracy: 0.4776\n",
            "Epoch 6/15\n",
            "417/417 [==============================] - 4s 8ms/step - loss: 249.7624 - accuracy: 0.4776\n",
            "Epoch 7/15\n",
            "417/417 [==============================] - 4s 8ms/step - loss: 249.7624 - accuracy: 0.4776\n",
            "Epoch 8/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7623 - accuracy: 0.4776\n",
            "Epoch 9/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7623 - accuracy: 0.4776\n",
            "Epoch 10/15\n",
            "417/417 [==============================] - 4s 8ms/step - loss: 249.7622 - accuracy: 0.4776\n",
            "Epoch 11/15\n",
            "417/417 [==============================] - 4s 8ms/step - loss: 249.7623 - accuracy: 0.4776\n",
            "Epoch 12/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7622 - accuracy: 0.4776\n",
            "Epoch 13/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 249.7622 - accuracy: 0.4776\n",
            "Epoch 14/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 249.7625 - accuracy: 0.4776\n",
            "Epoch 15/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 249.7622 - accuracy: 0.4776\n",
            "26748/26748 [==============================] - 68s 3ms/step - loss: 250.4004 - accuracy: 0.4773\n",
            "Evaluation individu:  3\n",
            "Epoch 1/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 250.4576 - accuracy: 0.5224\n",
            "Epoch 2/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 250.4574 - accuracy: 0.5224\n",
            "Epoch 3/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4575 - accuracy: 0.5224\n",
            "Epoch 4/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4575 - accuracy: 0.5224\n",
            "Epoch 5/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 250.4575 - accuracy: 0.5224\n",
            "Epoch 6/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 250.4576 - accuracy: 0.5224\n",
            "Epoch 7/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4574 - accuracy: 0.5224\n",
            "Epoch 8/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4575 - accuracy: 0.5224\n",
            "Epoch 9/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 250.4575 - accuracy: 0.5224\n",
            "Epoch 10/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4576 - accuracy: 0.5224\n",
            "Epoch 11/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4577 - accuracy: 0.5224\n",
            "Epoch 12/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4574 - accuracy: 0.5224\n",
            "Epoch 13/15\n",
            "417/417 [==============================] - 4s 9ms/step - loss: 250.4576 - accuracy: 0.5224\n",
            "Epoch 14/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4576 - accuracy: 0.5224\n",
            "Epoch 15/15\n",
            "417/417 [==============================] - 3s 8ms/step - loss: 250.4575 - accuracy: 0.5224\n",
            "26748/26748 [==============================] - 72s 3ms/step - loss: 250.4379 - accuracy: 0.5227\n",
            "Evaluation individu:  4\n",
            "Epoch 1/15\n",
            "417/417 [==============================] - 17s 38ms/step - loss: 249.2272 - accuracy: 0.5223\n",
            "Epoch 2/15\n",
            "417/417 [==============================] - 15s 37ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 3/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 4/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 5/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7624 - accuracy: 0.5224\n",
            "Epoch 6/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 7/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 8/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 9/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 10/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 11/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 12/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 13/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 14/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7623 - accuracy: 0.5224\n",
            "Epoch 15/15\n",
            "417/417 [==============================] - 15s 36ms/step - loss: 249.7622 - accuracy: 0.5224\n",
            "26748/26748 [==============================] - 82s 3ms/step - loss: 250.4004 - accuracy: 0.5227\n",
            "Evaluation individu:  5\n",
            "Epoch 1/15\n",
            "417/417 [==============================] - 13s 27ms/step - loss: 21.6135 - accuracy: 0.5058\n",
            "Epoch 2/15\n",
            "417/417 [==============================] - 11s 27ms/step - loss: 21.4741 - accuracy: 0.4995\n",
            "Epoch 3/15\n",
            "417/417 [==============================] - 11s 27ms/step - loss: 21.4735 - accuracy: 0.5027\n",
            "Epoch 4/15\n",
            "417/417 [==============================] - 11s 27ms/step - loss: 21.4732 - accuracy: 0.5000\n",
            "Epoch 5/15\n",
            "417/417 [==============================] - 11s 27ms/step - loss: 21.4731 - accuracy: 0.5001\n",
            "Epoch 6/15\n",
            "417/417 [==============================] - 11s 27ms/step - loss: 21.4730 - accuracy: 0.5041\n",
            "Epoch 7/15\n",
            "417/417 [==============================] - 11s 27ms/step - loss: 21.4729 - accuracy: 0.5011\n",
            "Epoch 8/15\n",
            " 12/417 [..............................] - ETA: 10s - loss: 21.3571 - accuracy: 0.5189"
          ]
        }
      ],
      "source": [
        "NB_OF_GENERATION = 10\n",
        "POPULATION_SIZE = 20\n",
        "NB_PARENTS = 15\n",
        "INPUT_SHAPE =(32,32,1)\n",
        "BATCH_SIZE = 150\n",
        "NB_EPOCHS = 15\n",
        "ELITE_FRAC = 0.5\n",
        "CHILDREN_FRAC = 0.5\n",
        "TEST_SIZE = 0.4\n",
        "PROBA_MUTATION = 0.7\n",
        "PROBA_CROSSOVER = 0.7\n",
        "\n",
        "\n",
        "date = datetime.datetime.now()\n",
        "date = date.strftime(\"%m_%d_%H_%M_%S\")\n",
        "\n",
        "FILE_NAME1 = f\"file_{date}.txt\"\n",
        "FILE_NAME2 = f\"file_{date}.csv\"\n",
        "\n",
        "f = open(FILE_NAME1,\"w\")\n",
        "f.write(f\"NB_OF_GENERATION = {NB_OF_GENERATION}\\nPOPULATION_SIZE = {POPULATION_SIZE}\\nINPUT_SHAPE \"+\n",
        "        f\"= {INPUT_SHAPE}\\nBATCH_SIZE = {BATCH_SIZE}\\nNB_EPOCHS = {NB_EPOCHS}\\nELITE_FRAC = {ELITE_FRAC}\\nCHILDREN_FRAC = \"+\n",
        "        f\"{CHILDREN_FRAC}\\nTEST_SIZE = {TEST_SIZE}\\nPROBA_MUTATION = {PROBA_MUTATION}\\nPROBA_CROSSOVER = {PROBA_CROSSOVER}\")\n",
        "\n",
        "f.close()\n",
        "\n",
        "if not os.path.isfile(FILE_NAME2):\n",
        "    columns = [\"test accuracy\", \"time\"]\n",
        "    csv_file = open(FILE_NAME2, 'w',newline='')\n",
        "    writer = csv.DictWriter(csv_file,fieldnames=columns)\n",
        "    writer.writeheader()\n",
        "    csv_file.close()\n",
        "\n",
        "csv_file = \"/content/Dataset/BaseDeDonnees_2/Labels.csv\"\n",
        "image_path = \"/content/Dataset/BaseDeDonnees_2/Images\" \n",
        "\n",
        "TrainSet_X,TestSet_X,TrainSet_Y, TestSet_Y = LoadDataBase2(csv_file,image_path)\n",
        "\n",
        "\n",
        "optimizer = SGD(0.02)\n",
        "best_solution = GeneticAlgorithme(POPULATION_SIZE,NB_OF_GENERATION,NB_PARENTS,ELITE_FRAC,CHILDREN_FRAC,optimizer,\n",
        "                INPUT_SHAPE,[TrainSet_X,TrainSet_Y],[TestSet_X,TestSet_Y],NB_EPOCHS,BATCH_SIZE, FILE_NAME1,FILE_NAME2,PROBA_CROSSOVER,PROBA_MUTATION)\n",
        "print(\"Best Solution: \",best_solution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA9LRgBygXU8"
      },
      "outputs": [],
      "source": [
        "!cp -r \"file_{date}.txt\" \"/content/drive/MyDrive/Colab Notebooks/Projet Master/Tests/file_{date}.txt\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}